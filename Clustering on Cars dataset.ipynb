{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "# AUTOMOTIVE INDUSTRY\n\nProblem to be solved: Determine which existing vehicles on the market are most like the prototypes on automobile manufacturer has developed for a new vehicle; which models the manufacturer will be competing against. The goal is to summarize the existing vehicles and help manufacturers to make decisions about the supply of new models.\n\n## The dataset\n\nWe have downloaded the car dataset, Car_sales.csv, which include information about different cars . This data set is being taken from Kaggle. [Dataset source](https://www.kaggle.com/gagandeep16/car-sales)\n\nThese are the columns of the dataset:\n- **Manufacturer**\n- **Model**\n- **Sales_in_thousands**\n- **__year_resale_value**\n- **Vehicle_type**\n- **Price_in_thousands**\n- **Engine_size**\n- **Horsepower**\n- **Wheelbase**\n- **Width**\n- **Length**\n- **Curb_weight**\n- **Fuel_capacity**\n- **Fuel_efficiency**\n- **Latest_Launch**\n- **Power_perf_factor**"}, {"metadata": {}, "cell_type": "markdown", "source": "## Explore the dataset\n\nImporte Pandas,Numpy and other necessary libraries. Open the dataset which is in csv format using the pd.read_csv() method of Pandas. After that, check the first rows of the dataframe."}, {"metadata": {}, "cell_type": "code", "source": "# importing necessary libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns # Visualization on top of matplotlib", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#import data\nimport types\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\nclient_01cc527bb96d4bf4a425a055e527cada = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='b82ZZUzL652upPKrBDIIzVcumAE--dvhoXpzzes8FBFs',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n\nbody = client_01cc527bb96d4bf4a425a055e527cada.get_object(Bucket='mlprojectsforvariousindustries-donotdelete-pr-cswviijcwiynz0',Key='Car_sales.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\n# If you are reading an Excel file into a pandas DataFrame, replace `read_csv` by `read_excel` in the next statement.\ndf = pd.read_csv(body)\ndf.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now use the **df.info()** method to get the better idea of the dataset."}, {"metadata": {}, "cell_type": "code", "source": "df.info()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Data cleaning\n\nLets see the size of the dataframe with the **df.shape** method. (Number of rows, number of columns)"}, {"metadata": {}, "cell_type": "code", "source": "df.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Check for duplicade rows and drop them if exist, by keeping the last found."}, {"metadata": {}, "cell_type": "code", "source": "df.drop_duplicates(keep ='last')\ndf.shape", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Since the shape of the dataset didn't change, means the raw dataset don't have duplicate row/values.\n\nNow, use **df.dtypes** method to check out the data type of each column in the dataset."}, {"metadata": {}, "cell_type": "code", "source": "df.dtypes", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Lets simply clear the dataset by dropping the rows that have null value:"}, {"metadata": {}, "cell_type": "code", "source": "print (\"Shape of dataset before cleaning: \", df.size)\ndf[[ 'Sales_in_thousands', '__year_resale_value', 'Price_in_thousands', 'Engine_size',\n       'Horsepower', 'Wheelbase', 'Width', 'Length', 'Curb_weight', 'Fuel_capacity',\n       'Fuel_efficiency', 'Power_perf_factor']] = df[['Sales_in_thousands', '__year_resale_value', 'Price_in_thousands', 'Engine_size',\n       'Horsepower', 'Wheelbase', 'Width', 'Length', 'Curb_weight', 'Fuel_capacity',\n       'Fuel_efficiency', 'Power_perf_factor']].apply(pd.to_numeric, errors='coerce')\ndf = df.dropna()\ndf = df.reset_index(drop=True)\nprint (\"Shape of dataset after cleaning: \", df.size)\ndf.head(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Finding maximum/minimum values through function. Apply it on feature 'Sales_in_thousands'."}, {"metadata": {}, "cell_type": "markdown", "source": "## Analysis of the data and Visualizations\n\nUse the **df.describe()** method, which computes the basic statistics for all continuous variables.\n\nThe result will contain the following things:\n\n- the count or frequency.\n- the mean or average.\n- the standard deviation (std).\n- the minimum value\n- the IQR (Interquartile Range: 25%, 50% and 75%)\n- the maximum value\n"}, {"metadata": {}, "cell_type": "code", "source": "df.describe()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Additionly, use the **df.describe()** method with an additional argument in it \" include = 'all' \" , which computes the basic statistics for all continuous variables as well as some statistics for the categorical variables.\n\nFew additional things will be added:\n\n- The number of unique values (unique)\n- The most frequent value (top)\n- The frequency of the top element (freq)"}, {"metadata": {}, "cell_type": "code", "source": "df.describe(include = 'all')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Correlation\n\nA mutual interdependence between two or more things is known as correlation. Check the Correlation between the continuous variables using the **df.corr()** method.\n"}, {"metadata": {}, "cell_type": "code", "source": "tmp = df.corr()\ntmp", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Heat Map\n\nDraw the heatmap of the columns below.\n"}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.heatmap(df[['Sales_in_thousands', '__year_resale_value', 'Price_in_thousands', 'Engine_size',\n       'Horsepower', 'Wheelbase', 'Width', 'Length', 'Curb_weight', 'Fuel_capacity',\n       'Fuel_efficiency', 'Power_perf_factor']].corr(),cmap='coolwarm',annot=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Lets focues on the correlations of features with Price_in_thousands column."}, {"metadata": {}, "cell_type": "code", "source": "df_corr = df.corr()['Price_in_thousands'][:-1]\ndf_corr", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We see a strong correlation of Price_in_thousands with _year_resale_value and Hoursepower.\n\nLets look at the pairplotfocusing again on Price_in_thousands column."}, {"metadata": {}, "cell_type": "code", "source": "for i in range(0, len(df.columns),5):\n    sns.pairplot(df, y_vars ='Price_in_thousands', x_vars = df.columns[i:i+5]  )", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def max_min_val(col):\n    '''\n    This function takes the column name as the argument\n    and returns the top and bottom observations in the dataframe\n    '''\n    first = df[col].idxmax()\n    first_obs = pd.DataFrame(df.loc[first])\n    \n    last = df[col].idxmin()\n    last_obs = pd.DataFrame(df.loc[last])\n    \n    min_max_obs = pd.concat([first_obs, last_obs], axis=1)\n    \n    return min_max_obs\n\nmax_min_val('Sales_in_thousands')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The Fold F-Series has the most sales, while the Mitsubishi 3000GT has the fewest."}, {"metadata": {}, "cell_type": "markdown", "source": "Count number of cars grouped by Manufacturer."}, {"metadata": {}, "cell_type": "code", "source": "make_dist = df.groupby('Manufacturer').size()\nmake_dist", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Ford has 10 vehicles on the clean dataset, with Dodge following with 9 and Toyota with Chervolet following with 8."}, {"metadata": {}, "cell_type": "code", "source": "make_dist.plot(title = 'Make Distribution')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Creating histogram for continuous numerical variable."}, {"metadata": {}, "cell_type": "code", "source": "viz = df[['Sales_in_thousands', '__year_resale_value', 'Price_in_thousands', 'Engine_size',\n       'Horsepower', 'Wheelbase', 'Width', 'Length', 'Curb_weight', 'Fuel_capacity',\n       'Fuel_efficiency', 'Power_perf_factor']]\nviz.hist()\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Preparation for Clustering\n\n#### Feature selection\nLets select our feature set:"}, {"metadata": {}, "cell_type": "code", "source": "featureset = df[['Engine_size',  'Horsepower', 'Wheelbase', 'Width', 'Length', 'Curb_weight', 'Fuel_capacity', 'Fuel_efficiency']]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Normalization\nNow we can normalize the feature set. __MinMaxScaler__ transforms features by scaling each feature to a given range. It is by default (0, 1). That is, this estimator scales and translates each feature individually such that it is between zero and one."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.preprocessing import MinMaxScaler\nx = featureset.values #returns a numpy array\nmin_max_scaler = MinMaxScaler()\nfeature_mtx = min_max_scaler.fit_transform(x)\nfeature_mtx [0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<h2 id=\"clustering_using_scipy\">Clustering using Scipy</h2>\nIn this part we use Scipy package to cluster the dataset:  \nFirst, we calculate the distance matrix. "}, {"metadata": {}, "cell_type": "code", "source": "import scipy\nleng = feature_mtx.shape[0]\nD = scipy.zeros([leng,leng])\nfor i in range(leng):\n    for j in range(leng):\n        D[i,j] = scipy.spatial.distance.euclidean(feature_mtx[i], feature_mtx[j])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In agglomerative clustering, at each iteration, the algorithm must update the distance matrix to reflect the distance of the newly formed cluster with the remaining clusters in the forest. The following methods are supported in Scipy for calculating the distance between the newly formed cluster and each: - single - complete - average - weighted - centroid."}, {"metadata": {}, "cell_type": "code", "source": "import pylab\nimport scipy.cluster.hierarchy\nZ = scipy.cluster.hierarchy.linkage(D, 'complete')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Essentially, Hierarchical clustering does not require a pre-specified number of clusters. However, in some applications we want a partition of disjoint clusters just as in flat clustering.\nSo you can use a cutting line:"}, {"metadata": {}, "cell_type": "code", "source": "from scipy.cluster.hierarchy import fcluster\nmax_d = 3\nclusters = fcluster(Z, max_d, criterion='distance')\nclusters", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Also, you can determine the number of clusters directly:"}, {"metadata": {}, "cell_type": "code", "source": "from scipy.cluster.hierarchy import fcluster\nk = 5\nclusters = fcluster(Z, k, criterion='maxclust')\nclusters", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, plot the dendrogram:"}, {"metadata": {}, "cell_type": "code", "source": "ig = pylab.figure(figsize=(18,50))\ndef llf(id):\n    return '[%s %s %s]' % (df['Manufacturer'][id], df['Model'][id], df['Vehicle_type'][id])\n    \ndendro = scipy.cluster.hierarchy.dendrogram(Z,  leaf_label_func=llf, leaf_rotation=0, leaf_font_size =12, orientation = 'right')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Clustering using scikit-learn\nLets redo it again, but this time using scikit-learn package:"}, {"metadata": {}, "cell_type": "code", "source": "from scipy.spatial import distance_matrix \nfrom matplotlib import pyplot as plt \nfrom sklearn import manifold, datasets \nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.datasets.samples_generator import make_blobs ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "dist_matrix = distance_matrix(feature_mtx,feature_mtx) \nprint(dist_matrix)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, we can use the 'AgglomerativeClustering' function from scikit-learn library to cluster the dataset. The AgglomerativeClustering performs a hierarchical clustering using a bottom up approach. The linkage criteria determines the metric used for the merge strategy:\n\n- Ward minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach.\n- Maximum or complete linkage minimizes the maximum distance between observations of pairs of clusters.\n- Average linkage minimizes the average of the distances between all observations of pairs of clusters."}, {"metadata": {}, "cell_type": "code", "source": "agglom = AgglomerativeClustering(n_clusters = 6, linkage = 'complete')\nagglom.fit(feature_mtx)\nagglom.labels_", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "And, we can add a new field to our dataframe to show the cluster of each row:"}, {"metadata": {}, "cell_type": "code", "source": "df['cluster_'] = agglom.labels_\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.cm as cm\nn_clusters = max(agglom.labels_)+1\ncolors = cm.rainbow(np.linspace(0, 1, n_clusters))\ncluster_labels = list(range(0, n_clusters))\n\n# Create a figure of size 6 inches by 4 inches.\nplt.figure(figsize=(16,14))\n\nfor color, label in zip(colors, cluster_labels):\n    subset = df[df.cluster_ == label]\n    for i in subset.index:\n            plt.text(subset.Horsepower[i], subset.Fuel_efficiency[i],str(subset['Model'][i]), rotation=25) \n    plt.scatter(subset.Horsepower, subset.Fuel_efficiency, s= subset.Price_in_thousands*10, c=color, label='cluster'+str(label),alpha=0.5)\n#    plt.scatter(subset.Horsepower, subset.Fuel_efficiency)\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('Horsepower')\nplt.ylabel('Fuel_efficiency')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see, we are seeing the distribution of each cluster using the scatter plot, but it is not very clear where is the centroid of each cluster. Moreover, there are 2 types of vehicles in our dataset, \"Passenger\" and \"Car\". So, we use them to distinguish the classes, and summarize the cluster. First we count the number of cases in each group:"}, {"metadata": {}, "cell_type": "code", "source": "df.groupby(['cluster_','Vehicle_type'])['cluster_'].count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we can look at the characteristics of each cluster:"}, {"metadata": {}, "cell_type": "code", "source": "agg_cars = df.groupby(['cluster_','Vehicle_type'])['Horsepower','Engine_size','Fuel_efficiency','Price_in_thousands'].mean()\nagg_cars", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "It is obvious that we have 3 main clusters with the majority of vehicles in those.\n\n__Cars__:\n- Cluster 1: with almost high Fuel_efficiency, and low in Horsepower.\n- Cluster 2: with good Fuel_efficiency and Horsepower, but higher Price_in_thousands than average.\n- Cluster 3: with low Fuel_efficiency, high Horsepower, highest Price_in_thousands.\n    \n    \n    \n__Passenger__:\n- Cluster 1: with almost highest Fuel_efficiency among trucks, and lowest in Horsepower and Price_in_thousands.\n- Cluster 2: with almost low Fuel_efficiency and medium Horsepower, but higher Price_in_thousands than average.\n- Cluster 3: with good Fuel_efficiency and Horsepower, low Price_in_thousands.\n\n\nPlease notice that we did not use __Vehicle_type__ , and __Price_in_thousands__ of cars in the clustering process, but Hierarchical clustering could forge the clusters and discriminate them with quite high accuracy."}, {"metadata": {}, "cell_type": "code", "source": "plt.figure(figsize=(16,10))\nfor color, label in zip(colors, cluster_labels):\n    subset = agg_cars.loc[(label,),]\n    for i in subset.index:\n        plt.text(subset.loc[i][0]+5, subset.loc[i][2], 'Vehicle_type='+str(i) + ', Price_in_thousands='+str(int(subset.loc[i][3]))+'k')\n    plt.scatter(subset.Horsepower, subset.Fuel_efficiency, s=subset.Price_in_thousands*20, c=color, label='cluster'+str(label))\nplt.legend()\nplt.title('Clusters')\nplt.xlabel('Horsepower')\nplt.ylabel('Fuel_efficiency')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.6", "language": "python"}, "language_info": {"name": "python", "version": "3.6.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}